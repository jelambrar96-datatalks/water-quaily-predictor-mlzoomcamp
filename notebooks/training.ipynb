{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as  pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://raw.githubusercontent.com/Sarthak-1408/Water-Potability/refs/heads/main/water_potability.csv\"\n",
    "DATASET_FILEPATH = \"./water_potability.csv\"\n",
    "if not os.path.isfile(DATASET_FILEPATH):\n",
    "    response = requests.get(URL)\n",
    "    # Check if the download was successful\n",
    "    if response.status_code == 200:\n",
    "        with open('water_potability.csv', 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        print(\"CSV file downloaded successfully.\")\n",
    "    else:\n",
    "        raise Exception(f\"Failed to download file. Status code: {response.status_code}\")\n",
    "\n",
    "df = pd.read_csv(DATASET_FILEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.sample(100, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Transform Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ph', 'hardness', 'solids', 'chloramines', 'sulfate', 'conductivity', 'organic_carbon', 'trihalomethanes', 'turbidity', 'potability']\n"
     ]
    }
   ],
   "source": [
    "COLUMNS = list(df.columns)\n",
    "print(COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COLUMN = 'potability'\n",
    "COLUMNS.remove(TARGET_COLUMN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ph</th>\n",
       "      <th>hardness</th>\n",
       "      <th>solids</th>\n",
       "      <th>chloramines</th>\n",
       "      <th>sulfate</th>\n",
       "      <th>conductivity</th>\n",
       "      <th>organic_carbon</th>\n",
       "      <th>trihalomethanes</th>\n",
       "      <th>turbidity</th>\n",
       "      <th>potability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>6.063355</td>\n",
       "      <td>160.767311</td>\n",
       "      <td>14775.145596</td>\n",
       "      <td>7.484104</td>\n",
       "      <td>305.828553</td>\n",
       "      <td>327.270239</td>\n",
       "      <td>12.309016</td>\n",
       "      <td>69.038454</td>\n",
       "      <td>3.467337</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>NaN</td>\n",
       "      <td>233.020134</td>\n",
       "      <td>27071.118618</td>\n",
       "      <td>6.220936</td>\n",
       "      <td>298.112645</td>\n",
       "      <td>357.119622</td>\n",
       "      <td>16.768945</td>\n",
       "      <td>51.284401</td>\n",
       "      <td>4.284879</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1784</th>\n",
       "      <td>9.252857</td>\n",
       "      <td>168.040751</td>\n",
       "      <td>50279.262429</td>\n",
       "      <td>5.905056</td>\n",
       "      <td>415.450810</td>\n",
       "      <td>400.003589</td>\n",
       "      <td>11.949854</td>\n",
       "      <td>62.256881</td>\n",
       "      <td>3.300586</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3061</th>\n",
       "      <td>3.989032</td>\n",
       "      <td>216.076138</td>\n",
       "      <td>40175.206091</td>\n",
       "      <td>7.487423</td>\n",
       "      <td>309.800796</td>\n",
       "      <td>399.047181</td>\n",
       "      <td>11.186739</td>\n",
       "      <td>86.957464</td>\n",
       "      <td>4.409414</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2779</th>\n",
       "      <td>9.454119</td>\n",
       "      <td>224.817132</td>\n",
       "      <td>21379.963927</td>\n",
       "      <td>5.407692</td>\n",
       "      <td>227.665635</td>\n",
       "      <td>431.613001</td>\n",
       "      <td>15.772334</td>\n",
       "      <td>52.033845</td>\n",
       "      <td>4.058626</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ph    hardness        solids  chloramines     sulfate  \\\n",
       "136   6.063355  160.767311  14775.145596     7.484104  305.828553   \n",
       "600        NaN  233.020134  27071.118618     6.220936  298.112645   \n",
       "1784  9.252857  168.040751  50279.262429     5.905056  415.450810   \n",
       "3061  3.989032  216.076138  40175.206091     7.487423  309.800796   \n",
       "2779  9.454119  224.817132  21379.963927     5.407692  227.665635   \n",
       "\n",
       "      conductivity  organic_carbon  trihalomethanes  turbidity  potability  \n",
       "136     327.270239       12.309016        69.038454   3.467337           0  \n",
       "600     357.119622       16.768945        51.284401   4.284879           0  \n",
       "1784    400.003589       11.949854        62.256881   3.300586           0  \n",
       "3061    399.047181       11.186739        86.957464   4.409414           0  \n",
       "2779    431.613001       15.772334        52.033845   4.058626           1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 100 entries, 136 to 613\n",
      "Data columns (total 10 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   ph               87 non-null     float64\n",
      " 1   hardness         100 non-null    float64\n",
      " 2   solids           100 non-null    float64\n",
      " 3   chloramines      100 non-null    float64\n",
      " 4   sulfate          75 non-null     float64\n",
      " 5   conductivity     100 non-null    float64\n",
      " 6   organic_carbon   100 non-null    float64\n",
      " 7   trihalomethanes  99 non-null     float64\n",
      " 8   turbidity        100 non-null    float64\n",
      " 9   potability       100 non-null    int64  \n",
      "dtypes: float64(9), int64(1)\n",
      "memory usage: 8.6 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Remove nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_full_train, df_test = train_test_split(df, test_size=0.2, random_state=1)\n",
    "df_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=1)\n",
    "\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "\n",
    "y_train = (df_train[TARGET_COLUMN]).astype('int').values\n",
    "y_val = (df_val[TARGET_COLUMN]).astype('int').values\n",
    "y_test = (df_test[TARGET_COLUMN]).astype('int').values\n",
    "\n",
    "del df_train[TARGET_COLUMN]\n",
    "del df_val[TARGET_COLUMN]\n",
    "del df_test[TARGET_COLUMN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "dv = DictVectorizer(sparse=False)\n",
    "\n",
    "train_dict = df_train.to_dict(orient='records')\n",
    "X_train = dv.fit_transform(train_dict)\n",
    "\n",
    "val_dict = df_val.to_dict(orient='records')\n",
    "X_val = dv.transform(val_dict)\n",
    "\n",
    "test_dict = df_test.to_dict(orient='records')\n",
    "X_test = dv.transform(test_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Save pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from typing import Any, Optional\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import os\n",
    "\n",
    "\n",
    "def save_to_pickle(\n",
    "    obj: Any,\n",
    "    file_path: str,\n",
    "    create_dir: bool = True,\n",
    "    compression: Optional[str] = None\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Save an object to a pickle file with error handling.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    obj : Any\n",
    "        The Python object to save\n",
    "    file_path : str\n",
    "        Path where the pickle file will be saved\n",
    "    create_dir : bool, optional (default=True)\n",
    "        If True, creates the directory if it doesn't exist\n",
    "    compression : str, optional (default=None)\n",
    "        Compression protocol to use ('gzip', 'bz2', 'lzma' or None)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    bool\n",
    "        True if save was successful, False otherwise\n",
    "        \n",
    "    Examples:\n",
    "    --------\n",
    "    >>> data = {'key': 'value'}\n",
    "    >>> save_to_pickle(data, 'data/my_dict.pkl')\n",
    "    >>> save_to_pickle(data, 'data/my_dict.pkl.gz', compression='gzip')\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert to Path object for better path handling\n",
    "        path = Path(file_path)\n",
    "        \n",
    "        # Create directory if it doesn't exist and create_dir is True\n",
    "        if create_dir:\n",
    "            path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "        # Determine the appropriate open function and mode\n",
    "        if compression:\n",
    "            if compression == 'gzip':\n",
    "                import gzip\n",
    "                open_func = gzip.open\n",
    "            elif compression == 'bz2':\n",
    "                import bz2\n",
    "                open_func = bz2.open\n",
    "            elif compression == 'lzma':\n",
    "                import lzma\n",
    "                open_func = lzma.open\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported compression format: {compression}\")\n",
    "            mode = 'wb'\n",
    "        else:\n",
    "            open_func = open\n",
    "            mode = 'wb'\n",
    "\n",
    "        # Save the object\n",
    "        with open_func(path, mode) as f:\n",
    "            pickle.dump(obj, f)\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_pickle(dv, './dv.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Trainnning models\n",
    "\n",
    "### 5.0. Utils functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detalles adicionales de evaluación\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    confusion_matrix\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, y_train, X_val, y_val, params):\n",
    "    \"\"\"\n",
    "    # Función para evaluar un conjunto de hiperparámetros\n",
    "    \"\"\"\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('logistic', model(**params))\n",
    "    ])    \n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_val_pred = pipeline.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    return accuracy, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def find_best_model(Model, parameter_grid, X_train, y_train, X_val, y_val, extra_parameters = {}, verbose = False):\n",
    "    best_accuracy = -np.inf\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "\n",
    "    parameter_labels = parameter_grid.keys()\n",
    "    parameter_values = parameter_grid.values()\n",
    "\n",
    "    for temp_parameter_iterable in product(*parameter_values):\n",
    "        params = { label:value for label, value in zip(parameter_labels, temp_parameter_iterable) }\n",
    "        if verbose:\n",
    "            print()\n",
    "            print(params)\n",
    "\n",
    "        # Evaluamos los parámetros\n",
    "        try:\n",
    "            accuracy, model = evaluate_model(\n",
    "                Model, X_train, y_train, X_val, y_val, params\n",
    "            )\n",
    "        except ValueError as ve:\n",
    "            if verbose:\n",
    "                print(ve)\n",
    "            continue\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"accuracy: {accuracy}\")\n",
    "        \n",
    "        # Actualizamos mejor modelo si es necesario\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_params = params\n",
    "            best_model = model\n",
    "\n",
    "    return best_model, best_params, best_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Configuring MLFLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom mlflow.tracking import MlflowClient\\n\\nTRACKING_SERVER_HOST = \"localhost\"\\nclient = MlflowClient(f\"http://{TRACKING_SERVER_HOST}:5000\")\\n\\nmlflow.set_experiment(\"wqm-exp-1\")\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "TRACKING_SERVER_HOST = \"localhost\"\n",
    "client = MlflowClient(f\"http://{TRACKING_SERVER_HOST}:5000\")\n",
    "\n",
    "mlflow.set_experiment(\"wqm-exp-1\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Model 1: Logistic regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "param_grid = {\n",
    "    'penalty':['l1','l2','elasticnet', None],\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'solver': ['lbfgs','newton-cg','liblinear','sag','saga'],\n",
    "    'max_iter'  : [100,1000,2500,5000]\n",
    "}\n",
    "\n",
    "best_model, best_params, val_accuracy = find_best_model(\n",
    "    Model=LogisticRegression,\n",
    "    parameter_grid=param_grid,\n",
    "    X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores Hiperparámetros:\n",
      "penalty: l1\n",
      "C: 1\n",
      "solver: liblinear\n",
      "max_iter: 100\n",
      "\n",
      "Accuracy en Validación: 0.5500\n"
     ]
    }
   ],
   "source": [
    "# Imprimir resultados de optimización\n",
    "print(\"Mejores Hiperparámetros:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")\n",
    "print(f\"\\nAccuracy en Validación: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy en Prueba: 0.8500\n"
     ]
    }
   ],
   "source": [
    "# Evaluar en conjunto de prueba\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"Accuracy en Prueba: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Informe de Clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      1.00      0.90        13\n",
      "           1       1.00      0.57      0.73         7\n",
      "\n",
      "    accuracy                           0.85        20\n",
      "   macro avg       0.91      0.79      0.81        20\n",
      "weighted avg       0.88      0.85      0.84        20\n",
      "\n",
      "\n",
      "Matriz de Confusión:\n",
      "[[13  0]\n",
      " [ 3  4]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nInforme de Clasificación:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "print(\"\\nMatriz de Confusión:\")\n",
    "print(confusion_matrix(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_pickle(best_model, './model_logistic_regression.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Model 2: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 500],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'bootstrap': [True, False],\n",
    "    'random_state': [1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "best_model, best_params, val_accuracy = find_best_model(\n",
    "    Model=RandomForestClassifier,\n",
    "    parameter_grid=param_grid,\n",
    "    X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy en Prueba: 0.6000\n"
     ]
    }
   ],
   "source": [
    "# Evaluar en conjunto de prueba\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"Accuracy en Prueba: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Informe de Clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.69      0.69        13\n",
      "           1       0.43      0.43      0.43         7\n",
      "\n",
      "    accuracy                           0.60        20\n",
      "   macro avg       0.56      0.56      0.56        20\n",
      "weighted avg       0.60      0.60      0.60        20\n",
      "\n",
      "\n",
      "Matriz de Confusión:\n",
      "[[9 4]\n",
      " [4 3]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nInforme de Clasificación:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "print(\"\\nMatriz de Confusión:\")\n",
    "print(confusion_matrix(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_pickle(best_model, './model_random_forest.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Model 3: Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'kernel': ['linear', 'rbf', 'sigmoid'],\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1, 10],\n",
    "    'degree': [2, 3, 4],  # only relevant for poly kernel\n",
    "    'class_weight': [None, 'balanced'],\n",
    "    'decision_function_shape': ['ovr', 'ovo']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "best_model, best_params, val_accuracy = find_best_model(\n",
    "    Model=SVC,\n",
    "    parameter_grid=param_grid,\n",
    "    X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy en Prueba: 0.5500\n"
     ]
    }
   ],
   "source": [
    "# Evaluar en conjunto de prueba\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"Accuracy en Prueba: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Informe de Clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.62      0.64        13\n",
      "           1       0.38      0.43      0.40         7\n",
      "\n",
      "    accuracy                           0.55        20\n",
      "   macro avg       0.52      0.52      0.52        20\n",
      "weighted avg       0.56      0.55      0.56        20\n",
      "\n",
      "\n",
      "Matriz de Confusión:\n",
      "[[8 5]\n",
      " [4 3]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nInforme de Clasificación:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "print(\"\\nMatriz de Confusión:\")\n",
    "print(confusion_matrix(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'kernel': ['poly'],\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1, 10],\n",
    "    'degree': [2, 3, 4],  # only relevant for poly kernel\n",
    "    'class_weight': [None, 'balanced'],\n",
    "    'decision_function_shape': ['ovr', 'ovo']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model, best_params, val_accuracy = find_best_model(\n",
    "    Model=SVC,\n",
    "    parameter_grid=param_grid,\n",
    "    X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy en Prueba: 0.3500\n"
     ]
    }
   ],
   "source": [
    "# Evaluar en conjunto de prueba\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"Accuracy en Prueba: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Informe de Clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        13\n",
      "           1       0.35      1.00      0.52         7\n",
      "\n",
      "    accuracy                           0.35        20\n",
      "   macro avg       0.17      0.50      0.26        20\n",
      "weighted avg       0.12      0.35      0.18        20\n",
      "\n",
      "\n",
      "Matriz de Confusión:\n",
      "[[ 0 13]\n",
      " [ 0  7]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nInforme de Clasificación:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "print(\"\\nMatriz de Confusión:\")\n",
    "print(confusion_matrix(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_pickle(best_model, './model_svm_forest.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Model 4: Native Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.1 Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'var_smoothing': np.logspace(-11, -1, 20),\n",
    "    'priors': [None]  # Can be extended with specific priors if needed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model, best_params, val_accuracy = find_best_model(\n",
    "    Model=GaussianNB,\n",
    "    parameter_grid=param_grid,\n",
    "    X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy en Prueba: 0.6500\n"
     ]
    }
   ],
   "source": [
    "# Evaluar en conjunto de prueba\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"Accuracy en Prueba: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Informe de Clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.77      0.74        13\n",
      "           1       0.50      0.43      0.46         7\n",
      "\n",
      "    accuracy                           0.65        20\n",
      "   macro avg       0.61      0.60      0.60        20\n",
      "weighted avg       0.64      0.65      0.64        20\n",
      "\n",
      "\n",
      "Matriz de Confusión:\n",
      "[[10  3]\n",
      " [ 4  3]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nInforme de Clasificación:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "print(\"\\nMatriz de Confusión:\")\n",
    "print(confusion_matrix(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_pickle(best_model, './model_gaussian_nb.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.1 Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'alpha': np.logspace(-3, 3, 20),\n",
    "    'binarize': [None, 0.0, 0.25, 0.5, 0.75, 1.0],  # Threshold for binarizing features\n",
    "    'fit_prior': [True, False]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model, best_params, val_accuracy = find_best_model(\n",
    "    Model=BernoulliNB,\n",
    "    parameter_grid=param_grid,\n",
    "    X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy en Prueba: 0.5000\n"
     ]
    }
   ],
   "source": [
    "# Evaluar en conjunto de prueba\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"Accuracy en Prueba: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Informe de Clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.54      0.58        13\n",
      "           1       0.33      0.43      0.38         7\n",
      "\n",
      "    accuracy                           0.50        20\n",
      "   macro avg       0.48      0.48      0.48        20\n",
      "weighted avg       0.53      0.50      0.51        20\n",
      "\n",
      "\n",
      "Matriz de Confusión:\n",
      "[[7 6]\n",
      " [4 3]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nInforme de Clasificación:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "print(\"\\nMatriz de Confusión:\")\n",
    "print(confusion_matrix(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_to_pickle(best_model, './model_bernoulli_nb.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
